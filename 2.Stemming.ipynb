{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank you all so veri much .', 'thank you to the academi .', 'thank you to all of you in thi room .', 'I have to congratul the other incred nomine thi year .', 'the reven wa the product of the tireless effort of an unbeliev cast and crew .', 'first off , to my brother in thi endeavor , mr. tom hardi .', 'tom , your talent on screen can onli be surpass by your friendship off screen … thank you for creat a t ranscend cinemat experi .', 'thank you to everybodi at fox and new regenc … my entir team .', 'I have to thank everyon from the veri onset of my career … To my parent ; none of thi would be possibl without you .', 'and to my friend , I love you dearli ; you know who you are .', \"and lastli , I just want to say thi : make the reven wa about man 's relationship to the natur world .\", 'A world that we collect felt in 2015 as the hottest year in record histori .', 'our product need to move to the southern tip of thi planet just to be abl to find snow .', 'climat chang is real , it is happen right now .', 'It is the most urgent threat face our entir speci , and we need to work collect togeth and stop procrastin .', 'We need to support leader around the world who do not speak for the big pollut , but who speak for all of human , for the indigen peopl of the world , for the billion and billion of underprivileg peopl out there who would be most affect by thi .', 'for our children ’ s children , and for those peopl out there whose voic have been drownedout by the polit of greed .', 'I thank you all for thi amaz award tonight .', 'let us not take thi planet for grant .', 'I do not take tonight for grant .', 'thank you so veri much .']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "with open('text.txt','r') as file:\n",
    "    paragraph = file.read()\n",
    "          \n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Stemming\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        sentences[i] = ' '.join(words)   \n",
    "        \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
