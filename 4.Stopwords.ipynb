{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank much .', 'Thank Academy .', 'Thank room .', 'I congratulate incredible nominees year .', 'The Revenant product tireless efforts unbelievable cast crew .', 'First , brother endeavor , Mr. Tom Hardy .', 'Tom , talent screen surpassed friendship screen … thank creating ranscendent cinematic experience .', 'Thank everybody Fox New Regency … entire team .', 'I thank everyone onset career … To parents ; none would possible without .', 'And friends , I love dearly ; know .', \"And lastly , I want say : Making The Revenant man 's relationship natural world .\", 'A world collectively felt 2015 hottest year recorded history .', 'Our production needed move southern tip planet able find snow .', 'Climate change real , happening right .', 'It urgent threat facing entire species , need work collectively together stop procrastinating .', 'We need support leaders around world speak big polluters , speak humanity , indigenous people world , billions billions underprivileged people would affected .', 'For children ’ children , people whose voices drownedout politics greed .', 'I thank amazing award tonight .', 'Let us take planet granted .', 'I take tonight granted .', 'Thank much .']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "with open('text.txt','r') as file:\n",
    "    paragraph = file.read()\n",
    "\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "    # Removing stopwords\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        sentences[i] = ' '.join(words)      \n",
    "        \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual way to define own stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank much .', 'Thank to the Academy .', 'Thank to of this room .', 'I have to congratulate the other incredible nominees this year .', 'The Revenant was the product of the tireless efforts of unbelievable cast crew .', 'First off , to my brother this endeavor , Mr. Tom Hardy .', 'Tom , your talent screen can only be surpassed by your friendship off screen … thank for creating a t ranscendent cinematic experience .', 'Thank to everybody at Fox New Regency … my entire team .', 'I have to thank everyone the onset of my career … To my parents ; none of this would be possible without .', 'And to my friends , I love dearly ; know who are .', \"And lastly , I just want to say this : Making The Revenant was about man 's relationship to the natural world .\", 'A world that we collectively felt 2015 the hottest year recorded history .', 'Our production needed to move to the southern tip of this planet just to be able to find snow .', 'Climate change is real , it is happening right now .', 'It is the most urgent threat facing our entire species , we need to work collectively together stop procrastinating .', 'We need to support leaders around the world who do not speak for the big polluters , but who speak for of humanity , for the indigenous people of the world , for the billions billions of underprivileged people out there who would be most affected by this .', 'For our children ’ s children , for those people out there whose voices have been drownedout by the politics of greed .', 'I thank for this amazing award tonight .', 'Let us not take this planet for granted .', 'I do not take tonight for granted .', 'Thank much .']\n"
     ]
    }
   ],
   "source": [
    "stop_words_lst = ['and', 'on', 'from', 'in', 'as', 'an', 'you', 'i', \"him\", \"she\",\"all\", \"so\", \"very\"]\n",
    "\n",
    "with open('text.txt','r') as file:\n",
    "    paragraph = file.read()\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "    # Removing stopwords\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        words = [word for word in words if word not in stop_words_lst]\n",
    "        sentences[i] = ' '.join(words)      \n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
